title: 写给青少年的网页爬虫学习指南
categories: 
- 少儿编程
tags: 
- 少儿编程
- STEAM
- Python
keywords: 猫老师少儿编程,少儿编程,编程,K12,STEAM,STEM,Python
date: 2019-1-12 9:00:00

---

>注意，这是一篇写给**青少年**的入门指南。
>我们不会做非常深入的探讨，而是浅尝辄止，给初学者描绘一个概貌和入门指引。

## 缘起

*那一天，人们终于回想起了被爬虫所支配的恐惧*

2019元旦期间，有位同学报名学习 Python，上来就问了我一个问题，“什么时候可以开始学习爬虫呢？”

老师当时就被震惊了。现在的小朋友果然不可以低估啊。要知道猫老师本猫都还没实际操作过爬虫呢。而00后10后已经在以此为目标了。我不禁想问，这位少年，看你骨骼清奇，我这里有一本金牌黑客秘籍要不要看下？

玩笑归玩笑。有明确目标的学习，效果会是出类拔萃的。虽然猫老师之前并未专门对这项技术的教学做过准备，但是只要同学有需要，我们就不会被问题难住。教学相长，为了不辜负同学们的期望，我们决定从制作一个例子开始。

## 什么是爬虫

网络爬虫（crawler），又被称为蜘蛛（spider）。是一种可以根据某种设定的规则自动运行，抓取互联网上的信息的程序。我们经常使用的百度、必应、谷歌这些搜索引擎，它们背后的重要支撑，就是网络爬虫。这些爬虫不知疲倦地抓取网站的信息并加以综合整理，最终成为帮助我们快速检索需要的信息的利器。

从这个简单的描述，我们就可以知道，一个爬虫至少需要三个要素：

1. 抓取信息
2. 数据存储和整理
3. 数据呈现

这几个方面，每一个点都可以引申更多更深入的技术，值得数十年如一日地潜心研究。我们没有办法一一介绍，这一点你只要想象一下这些搜索引擎公司，为什么能凭借这个技术成长为互联网巨头就明白了。

但是作为初学者，想通过一些简单的学习，大致了解爬虫的工作原理，甚至做一些简单的爬虫应用，还是有可能的。

<!-- more -->

## 爬虫的基本工作原理

同学们可能已经了解，互联网是通过超链接（URL）访问和相互联系的。比如新闻门户网站上就排布着大量隐藏在新闻标题背后的超链接。

想象一下，整个互联网就好比是一张……啊不对，是很多很多张通过各种方式连接在一起的蜘蛛网。现在假设你是一只蜘蛛，把你放在网上，需要把所有的蛛丝都走过一遍（看一遍所有网页），要怎么做呢？

首先，我们先随便从什么地方开始，爬过一根蛛丝，遇到分叉就按照某种规则选择其中一条。按照这种方式不停爬行，迟早有一天，整个网络都会被蜘蛛爬过一遍。

这就是爬虫的基本工作原理，也是爬虫为什么叫爬虫（或者蜘蛛）的原因。

网络爬虫跟这只蜘蛛的区别在于，爬过的地方需要有地方存储它看到的信息。通常，由于这些信息非常庞大，我们需要使用数据库来进行存储。

除了这种通用的爬虫，还有一些有特定目标的爬虫，比如在网络购物时，可能会需要用到的比价网站，就是通过爬虫，抓取所有购物网站的信息，对相同商品同一时间不同网站的价格，或者历史价格进行对比和分析。

所以爬虫技术也是大数据的基础之一。由此可见，如何对抓取到的数据进行存储、整理、分析和最终的呈现，也非常重要。

## 实现爬虫需要的能力、工具及学习路径

知道了爬虫的基本工作原理，用Python去实现它需要什么工具也就大体有些概念了。

- 编程基础知识
	- Python 编程基础
	- Python 网络编程
	- Python 模块使用
	- 数据结构和算法
- Web 开发基本知识
	- HTML/DOM
	- CSS
	- JavaScript
- 正则表达式
- 数据库基础
- ……

需要指出的是 Python 并非制作爬虫的唯一选择。你其实可以用任何喜欢的语言来实现。只是现阶段用 Python 来实现爬虫非常有利。一是因为 Python 的语法简洁易懂；二是已经有很多、很好用的专为 Python 爬虫设计的第三方模块（什么是模块，我们会在 Python 中级课程中讲授）。

Python 的学习路径，猫老师已经在《Python 是个什么东西？》有简单说明。有兴趣的同学可以回顾一下。简单来说就是首先学基础的语法，然后包含数据结构和算法的高级特性，接下来，针对性学习爬虫所需的具体技术和模块。

## 例子程序：最右神回复

有一个知名问答网站，网友们可以在上面提出任何你想问的问题，然后就会有热心的或者内行的网友认真书写回答。

在这些回答中，有一些短小精悍，回味无穷的回复，被网友们戏称“神回复”。

我们这一次的例子，就以它为“下手”的目标。通过一个爬虫，来方便的汇总这些“神回复”。

这个爬虫属于定向聚焦爬虫。针对特定网站，特定目的。于是我们需要找到初始页面、根据规则抓取答案、对答案进行筛选和存储、通过小程序进行前端呈现。

爬虫部分，因为非常简单，我们没有使用著名的 Scrapy 库，而是用相对较轻的 requests 库来执行 url 请求，用 lxml 库及正则表达式来解析抓取到的网页，并用 pymongo 进行数据存储和整理。

前端呈现，我们制作了一个微信小程序。这里又涉及了 WebApp、HTML/CSS、RestApi、Hash 等等知识。好在这不是我们的重点，你也完全可以不去考虑，仅仅在自己的个人电脑上输出数据，开心就好。

当然，也许有一天我们也会开始介绍如何进行前端的实现。在这里，就先[简单地体验一下吧](https://www.v2ex.com/t/522988)。

## 小结

这个小程序的最初版本，Jason老师和猫老师两个人花了大约两天时间，分别写出了前端（小程序）和后端（爬虫）。其中，爬虫部分的核心部分不超过一百行 Python 代码。但是，初学者仍需要从最基本的知识和概念学习，方能理解那短短几十行代码究竟干了些什么。

真正实用的爬虫，还需要考虑很多问题。比如，面对大量网站海量数据，如何提高抓取的效率；如何并行处理和存储；如何避免重复抓取，同时减少遗漏；如何将抓取的信息结构化存储；如何降低存储空间并提高查询性能和效率……不要怕，科技就是在不断发现、提问、研究、解决、迭代的过程中不断进步的。我们的学习也是一样。

最后，祝所有想要学习爬虫的同学都能早日实现自己的爬虫程序。
但是千万要注意，**请合理、合法地使用爬虫，并避免大量高频访问某个网站，不然不要怪老师没有警告你会被封IP、封账号哦！**

